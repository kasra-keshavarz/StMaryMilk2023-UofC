{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create MESH Drainage Database\n",
    "The purpose of this script is to calculate land cover fractions for each subbasin of interest. Then the landcover is converted the (subbasin*lc_types) and added to the driange database. <br>\n",
    "#### **Programmers**:\n",
    "Ala Bahrami <br>\n",
    "Cooper Albano <br>\n",
    "\n",
    "#### **Revision History**\n",
    "2021/04/22 -- (1) *Initial version created* <br>\n",
    "2021/05/04 -- (1) *Changed dimension name from 'lc_type' to 'ngru' and variable name from 'lc_frac' to 'GRU'*.<br>\n",
    "2021/05/04 -- (2) *Added LandUse variable*<br>\n",
    "2021/05/05 -- (1) *Append the LandUse information to drainage_ddb*<br>\n",
    "2021/05/06 -- (1) *Changed 'ngru' dimension to 'gru' to be consistent with MESH code*<br>\n",
    "2021/06/04 -- (1) *Modified I/O and variables for Fraser application*<br>\n",
    "2021/07/04 -- (1) *modified to the new_rank_extract function*<br>\n",
    "2022/06/23 -- (1) *Modified based on the new_rank_modi2 which is adaptable for multi-outlet*<br>\n",
    "2022/06/23 -- (2) *Consider the entire 19 land cover classes instead of regrouping*<br>\n",
    "2022/06/23 -- (3) *Save subbasin reordered metadata*<br>\n",
    "2022/06/23 -- (4) *Visualize and save subbasin selection for any outlet (optional)*<br>\n",
    "2022/06/26 -- (1) *Modified the way of reindexing the zonal histogram*<br>\n",
    "2022/08/25 -- (1) *Modify code to adapt NEXT variable having multiple outlets*<br>\n",
    "2022/11/01 -- (1) *Added a line to accept GIS tool .csv zonal hist as pandas dataframe*<br>\n",
    "2022/11/01 -- (2) *Added a line to rename prefix_0 to prefix_NOD for the GIS tool .csv zonal hist*<br>\n",
    "2022/11/01 -- (3) *Removed landcover fraction calculation for GIS tool .csv file*<br>\n",
    "2022/11/16 -- (1) *Removed .csv column reordering. No longer necessary due to gistool bug fix*<br>\n",
    "2022/11/23 -- (1) *Added functionality to read I/O from control file.*<br>\n",
    "2023/02/02 -- (1) added lines to convert the tosegment fill value to 0.<br>\n",
    "\n",
    "\n",
    "#### **Reference**\n",
    "#### **To Do:**\n",
    "1) The lc_types is based on NALCMS 2010. The name list is hard-coded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xs\n",
    "import pandas as pd\n",
    "from   datetime import date\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control File Handling\n",
    "The purpose of the control file is to provide all inputs to the scripts in the vector-based workflow to eliminate the need to alter the workflow scripts themselves. The following cells will retrieve settings from 'control_active.txt' and provide them as inputs to this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Access to the control file folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlFolder = Path('../0_control_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store the name of the 'active' file in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlFile = 'control_active.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to extract a given setting from the control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_control( file, setting ):\n",
    "     \n",
    "    # Open 'control_active.txt' and ...\n",
    "    with open(file) as contents:\n",
    "        for line in contents:\n",
    "             \n",
    "            # ... find the line with the requested setting\n",
    "            if setting in line and not line.startswith('#'):\n",
    "                break\n",
    "     \n",
    "    # Extract the setting's value\n",
    "    substring = line.split('|',1)[1]      # Remove the setting's name (split into 2 based on '|', keep only 2nd part)\n",
    "    substring = substring.split('#',1)[0] # Remove comments, does nothing if no '#' is found\n",
    "    substring = substring.strip()         # Remove leading and trailing whitespace, tabs, newlines\n",
    "        \n",
    "    # Return this value   \n",
    "    return substring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to specify a default path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_path(suffix):\n",
    "     \n",
    "    # Get the root path\n",
    "    rootPath = Path( read_from_control(controlFolder/controlFile,'root_path') )\n",
    "     \n",
    "    # Specify the forcing path\n",
    "    #defaultPath = rootPath / domainFolder / suffix\n",
    "    defaultPath = rootPath / suffix \n",
    "    return defaultPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the domain folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_name = read_from_control(controlFolder/controlFile,'domain_name')\n",
    "domainFolder = 'domain_' + domain_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the zonal statistics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_zh_path = read_from_control(controlFolder/controlFile,'input_lc_zh_path')\n",
    "lc_zh_name = read_from_control(controlFolder/controlFile,'input_lc_zh_name')\n",
    "\n",
    "# Specify default path if needed\n",
    "if lc_zh_path == 'default':\n",
    "    lc_zh_path = make_default_path('vector_based_workflow/workflow_data/domain_'+domain_name+'/zonalhist/') # outputs a Path()\n",
    "else:\n",
    "    lc_zh_path = Path(lc_zh_path) # make sure a user-specified path is a Path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the network topology file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_path = read_from_control(controlFolder/controlFile,'input_topo_path')\n",
    "topo_name = read_from_control(controlFolder/controlFile,'input_topo_name')\n",
    "\n",
    "# Specify default path if needed\n",
    "if topo_path == 'default':\n",
    "    topo_path = make_default_path('vector_based_workflow/workflow_data/domain_'+domain_name+'/topology/') # outputs a Path()\n",
    "else:\n",
    "    topo_path = Path(topo_path) # make sure a user-specified path is a Path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the basin shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merit_path = read_from_control(controlFolder/controlFile,'merit_basin_path')\n",
    "merit_name = read_from_control(controlFolder/controlFile,'merit_basin_name')\n",
    "\n",
    "# Specify default path if needed\n",
    "if merit_path == 'default':\n",
    "    merit_path = make_default_path('shape_file/catchment/') # outputs a Path()\n",
    "else:\n",
    "    merit_path = Path(merit_path) # make sure a user-specified path is a Path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = read_from_control(controlFolder/controlFile,'DDB_output_dir')\n",
    "\n",
    "# Specify default path if needed\n",
    "if outdir == 'default':\n",
    "    outdir = make_default_path('vector_based_workflow/workflow_data/domain_'+domain_name+'/drainagedatabase/') # outputs a Path()\n",
    "else:\n",
    "    outdir = Path(outdir) # make sure a user-specified path is a Path()\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "input_lc_zh              = lc_zh_path/lc_zh_name     \n",
    "input_topology           = topo_path/topo_name\n",
    "Merit_catchment_shape    = merit_path/merit_name\n",
    "domain_name              = read_from_control(controlFolder/controlFile,'domain_name')\n",
    "lc_type_prefix           = read_from_control(controlFolder/controlFile,'lc_type_prefix')\n",
    "tosegment_fill           = read_from_control(controlFolder/controlFile,'tosegment_fill_value')\n",
    "types                    = read_from_control(controlFolder/controlFile,'land_cover_types').split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function reindex to extract drainage database variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_rank_extract(input_topology): \n",
    "        #% Reading topology file and finding outlets\n",
    "        drainage_db = xs.open_dataset(input_topology)\n",
    "        drainage_db.close()\n",
    "\n",
    "        segid = drainage_db['seg_id'].values\n",
    "        tosegment = drainage_db['tosegment'].values\n",
    "\n",
    "        for i in range(0,len(tosegment)):\n",
    "            if tosegment[i] == tosegment_fill:\n",
    "                tosegment[i]=0\n",
    "\n",
    "        # Count the number of outlets\n",
    "        outlets = np.where(tosegment == 0)[0]\n",
    "\n",
    "        #% Search over to extract the subbasins drain into each outlet\n",
    "        rank_id_domain = np.array([]).astype(int)   \n",
    "        outlet_number = np.array([]).astype(int) \n",
    "        for k in range(len(outlets)):\n",
    "            # initial step \n",
    "            #segid_target = drainage_db['seg_id'].values[outlets[k]]\n",
    "            segid_target = segid[outlets[k]]\n",
    "            # set the rank of the outlet \n",
    "            rank_id = outlets[k]\n",
    "            \n",
    "            # find upstream segids drains into downstream\n",
    "            while(np.size(segid_target) >= 1): \n",
    "                if (np.size(segid_target) == 1):\n",
    "                    r = np.where(tosegment == segid_target)[0]\n",
    "                else:\n",
    "                    r = np.where(tosegment == segid_target[0])[0]    \n",
    "                # updated the target segid \n",
    "                segid_target = np.append(segid_target, segid[r])\n",
    "                # remove the first searched target\n",
    "                segid_target = np.delete(segid_target,0,0)\n",
    "                if (len(segid_target) == 0):\n",
    "                    break\n",
    "                # update the rank_id\n",
    "                rank_id = np.append(rank_id,r)\n",
    "            rank_id = np.flip(rank_id) \n",
    "            if (np.size(rank_id) > 1):\n",
    "                outlet_number = np.append(outlet_number, (k)*np.ones((len(rank_id),1)).astype(int))\n",
    "            else:\n",
    "                outlet_number = np.append(outlet_number, (k))\n",
    "            rank_id_domain = np.append(rank_id_domain, rank_id)\n",
    "            rank_id = []\n",
    "        #% reorder segid and tosegment \n",
    "        segid = segid[rank_id_domain]\n",
    "        tosegment = tosegment[rank_id_domain]         \n",
    "              \n",
    "        # rearrange outlets to be consistent with MESH outlet structure\n",
    "        # NB: In MESH outlets should be placed at the end of NEXT variable \n",
    "        NA = len(rank_id_domain)\n",
    "        fid1 = np.where(tosegment != 0)[0]\n",
    "        fid2 = np.where(tosegment == 0)[0]\n",
    "        fid =  np.append(fid1,fid2)\n",
    "        \n",
    "        rank_id_domain = rank_id_domain[fid]\n",
    "        segid =segid[fid]\n",
    "        tosegment = tosegment[fid]\n",
    "        outlet_number = outlet_number[fid]\n",
    "        \n",
    "        #% construct Rank and Next variables \n",
    "        Next = np.zeros(NA).astype(np.int32)\n",
    "        \n",
    "        for k in range(NA):\n",
    "            if (tosegment[k] != 0):\n",
    "                r = np.where(tosegment[k] == segid)[0] + 1 \n",
    "                Next[k] = r\n",
    "            else:\n",
    "                Next[k] = 0\n",
    "                \n",
    "        # Construct Rank from 1:NA\n",
    "        Rank = np.arange(1,NA+1).astype(np.int32)\n",
    "        \n",
    "        #% save subbasins reordered metadata \n",
    "        dt = {'Merit_reorderd_ID':rank_id_domain, 'Outlet_Number':outlet_number, \n",
    "              'Rank':Rank,'Next':Next,'Segid':segid,'tosegment':tosegment}\n",
    "        df = pd.DataFrame(data=dt, dtype = np.int64)\n",
    "        outrank = domain_name+'_Rank_ID'+'.csv'\n",
    "        df.to_csv(outdir/outrank, index=False)\n",
    "        \n",
    "        # % reordering network topology variables based on Rank 1:NA\n",
    "        for m in ['basin_area', 'length', 'slope', 'lon', 'lat', 'hruid', \n",
    "                  'seg_id', 'seg_hr_id', 'tosegment', 'width', 'manning']:\n",
    "            drainage_db[m].values = drainage_db[m].values[rank_id_domain]\n",
    "            \n",
    "        # % check if channel slope values match the minimum threshold \n",
    "        min_slope = 0.000001\n",
    "        drainage_db['slope'].values[drainage_db['slope'].values < min_slope] = min_slope\n",
    "        \n",
    "        # % Adding Rank and Next variables to the file\n",
    "        drainage_db['Rank'] = (['n'], Rank) \n",
    "        drainage_db['Rank'].attrs.update(standard_name = 'Rank', \n",
    "                            long_name = 'Element ID', units = '1', _FillValue = -1)\n",
    "        \n",
    "        drainage_db['Next'] = (['n'], Next) \n",
    "        drainage_db['Next'].attrs.update(standard_name = 'Next', \n",
    "                           long_name = 'Receiving ID', units = '1', _FillValue = -1)\n",
    "\n",
    "        # % Adding missing attributes and renaming variables\n",
    "        # Add 'axis' and missing attributes for the 'lat' variable.\n",
    "        drainage_db['lat'].attrs['standard_name'] = 'latitude'\n",
    "        drainage_db['lat'].attrs['units'] = 'degrees_north'\n",
    "        drainage_db['lat'].attrs['axis'] = 'Y'\n",
    "         \n",
    "        # Add 'axis' and missing attributes for the 'lon' variable.\n",
    "        drainage_db['lon'].attrs['standard_name'] = 'longitude'\n",
    "        drainage_db['lon'].attrs['units'] = 'degrees_east'\n",
    "        drainage_db['lon'].attrs['axis'] = 'X'\n",
    "         \n",
    "        # Add or overwrite 'grid_mapping' for each variable (except axes).\n",
    "        for v in drainage_db.variables:\n",
    "            if (drainage_db[v].attrs.get('axis') is None):\n",
    "                drainage_db[v].attrs['grid_mapping'] = 'crs'\n",
    "         \n",
    "        # Add the 'crs' itself (if none found).\n",
    "        if (drainage_db.variables.get('crs') is None):\n",
    "            drainage_db['crs'] = ([], np.int32(1))\n",
    "            drainage_db['crs'].attrs.update(grid_mapping_name = 'latitude_longitude', longitude_of_prime_meridian = 0.0, semi_major_axis = 6378137.0, inverse_flattening = 298.257223563)\n",
    "         \n",
    "        # Rename variables.\n",
    "        for old, new in zip(['basin_area', 'length', 'slope', 'manning'], ['GridArea', 'ChnlLength', 'ChnlSlope', 'R2N']):\n",
    "            drainage_db = drainage_db.rename({old: new})\n",
    "         \n",
    "        # Rename the 'subbasin' dimension (from 'n').\n",
    "        drainage_db = drainage_db.rename({'n': 'subbasin'})\n",
    "        \n",
    "        # % Specifying the NetCDF \"featureType\"\n",
    "        # Add a 'time' axis with static values set to today (in this case, time is not actually treated as a dimension).\n",
    "        drainage_db['time'] = (['subbasin'], np.zeros(len(rank_id_domain)))\n",
    "        drainage_db['time'].attrs.update(standard_name = 'time', units = ('days since %s 00:00:00' % date.today().strftime('%Y-%m-%d')), axis = 'T')\n",
    "         \n",
    "        # Set the 'coords' of the dataset to the new axes.\n",
    "        drainage_db = drainage_db.set_coords(['time', 'lon', 'lat'])\n",
    "         \n",
    "        # Add (or overwrite) the 'featureType' to identify the 'point' dataset.\n",
    "        drainage_db.attrs['featureType'] = 'point'\n",
    "        \n",
    "        return rank_id_domain, drainage_db, outlet_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_id_domain, drainage_db, outlet_number = new_rank_extract(input_topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the input zonal histogram of landcover and reindex it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'COMID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22055/1240082223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lc_zh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlc_zonal_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lc_zh\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# read GIS tool .csv zonal histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlc_zonal_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlc_zonal_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COMID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# sort by COMID for GIS tool zonal histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Zonal histogram not recognized.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6314\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6315\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6317\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/scipy-stack/2022a/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'COMID'"
     ]
    }
   ],
   "source": [
    "if str(input_lc_zh).endswith('.shp'):\n",
    "    lc_zonal_hist = gpd.read_file(input_lc_zh)                        # read QGIS .shp zonal histogram\n",
    "    lc_zonal_hist = lc_zonal_hist.sort_values(by=['COMID'])           # sort by COMID for QGIS zonal histogram\n",
    "elif str(input_lc_zh).endswith('.csv'):\n",
    "    lc_zonal_hist = pd.read_csv(input_lc_zh)                           # read GIS tool .csv zonal histogram\n",
    "    lc_zonal_hist = lc_zonal_hist.sort_values(by=['COMID'])           # sort by COMID for GIS tool zonal histogram\n",
    "else:\n",
    "    print('Zonal histogram not recognized.')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rename frac_0 to frac_NOD for compatibility with verify lc_types. \n",
    "###### *Note: does nothing for QGIS version of zonal stats*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(input_lc_zh).endswith('.csv'):\n",
    "    lc_zonal_hist = lc_zonal_hist.rename(columns={lc_type_prefix+'0':lc_type_prefix+'NOD'})\n",
    "    cols = lc_zonal_hist.columns.tolist()\n",
    "    for i in cols:\n",
    "        if lc_type_prefix in i:\n",
    "            if 'NOD' in i:\n",
    "                nod=i\n",
    "                cols.remove(i)\n",
    "                cols.append(nod)\n",
    "    lc_zonal_hist = lc_zonal_hist[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (OPTIONAL) Sanity check of the subbasin selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB: this section can be uncommented if a user want to do a sanity check of the subbasin selection \n",
    "## list of major segid_target outlet ids per each PFAF\n",
    "## {78011862 (Fraser), 78017388(columbia), 82000048(MRB), \n",
    "## 83012503, 71004266 (Hudson), 72039675 (St.Laurent), \n",
    "## Mississipi (74072586), 73017442, 81018374 (Yukon), 77032206,\n",
    "## 75022612, 75038087 (Hondo River), 75038096 (Usumacinta)}\n",
    "\n",
    "# shape_catchment = gpd.read_file(Merit_catchment_shape)\n",
    "# shape_catchment = shape_catchment.sort_values(by=['COMID'])\n",
    "# shape_catchment.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# segid = drainage_db['seg_id'].values\n",
    "# segid_target = 75038096 \n",
    "# r = np.where(segid == segid_target)[0] \n",
    "# r2 = np.where(outlet_number == outlet_number[r])[0]\n",
    "# rank_id = rank_id_domain[r2]\n",
    "\n",
    "# shape_catchment.loc[rank_id].plot(color='white', edgecolor='black')\n",
    "# shape_catchment.loc[rank_id].to_file(outdir+'PFAF_subselect_'+'%d'%segid_target+'.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Class Types\n",
    "###### *Use only LANDSAT or MODIS, not both.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *LANDSAT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: the NOD here represent the No-data. The NALCMS data has no-data category which its values is zero\n",
    "lc_type = np.array(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *MODIS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_type = np.array(['Evergreen Needleleaf Forests','Evergreen Broadleaf Forests','Deciduous Needleleaf Forests','Deciduous Broadleaf Forests',\n",
    "#            'Mixed Forests','Closed Shrublands','Open Shrublands', 'Woody Savannas',\n",
    "#            'Savannas','Grasslands','Permanent Wetlands','Croplands',\n",
    "#            'Urban and Built-up Lands','Cropland/Natural Vegetation Mosaics','Permanent Snow and Ice','Barren',\n",
    "#            'Water Bodies','No-data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify the list of lc_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(lc_type) + 1\n",
    "st = [];\n",
    "p  = [];\n",
    "for i in (range(1,m)):\n",
    "    if (i < m-1):\n",
    "        st1 =   lc_type_prefix+ str(i)\n",
    "    else:\n",
    "        st1 =   lc_type_prefix+ 'NOD' \n",
    "    \n",
    "    st = np.append(st, st1)\n",
    "    fid = np.where(lc_zonal_hist.columns == st1)[0]\n",
    "    if (fid.size == 0):\n",
    "        print ('land cover %s is not presented in the list of land cover for this PFAF' % lc_type[i-1])\n",
    "        p = np.int32(np.append(p, i-1))\n",
    "\n",
    "# add dummy land cover type required by MESH \n",
    "lc_type = np.append(lc_type, 'Dump')    \n",
    "\n",
    "# remove missing land cover types from  the list \n",
    "if (len(p) != 0) :\n",
    "    lc_type = np.delete(lc_type, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate land cover fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract land cover zonal hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_frac = lc_zonal_hist.filter(like=lc_type_prefix, axis = 1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Based on NALCMS LANDSAT data, the open water data are classified as No-DATA, so if the catchments have some no-data, users should verify if it falls inside the open-water that later be added to t he 'Water' land cover class.\n",
    "\n",
    "Here the it is required to add NALCMS-NOD to land class type of Water if No-data is included in lc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = np.where(lc_type == 'No-data')[0]\n",
    "if (fid.size != 0):\n",
    "    r1 = np.where(lc_type == 'Water')[0]\n",
    "    r2 = np.where(lc_type == 'No-data')[0]  \n",
    "    print(r1,r2)\n",
    "    # adding the nodata values to the water land cover type and drop it and remove from lc_type \n",
    "    lc_frac.values[:,r1] = lc_frac.values[:,r1] + lc_frac.values[:,r2]\n",
    "    lc_frac = lc_frac.drop(lc_frac.columns[r2], axis=1)\n",
    "    lc_type = np.delete(lc_type, r2)\n",
    "    \n",
    "# add Dump layer for MESH application\n",
    "lc_frac['Dump'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating land cover percentage. Only calculate if input zonal histogram is a shapefile (i.e. QGIS version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(input_lc_zh).endswith('.shp'):\n",
    "    lc_frac = lc_frac.apply(lambda x: round(x/x.sum(),2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the lc_frac as a dataset and save it as netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = drainage_db['lon'].values\n",
    "lat = drainage_db['lat'].values\n",
    "tt = drainage_db['time'].values\n",
    "\n",
    "lc_ds =  xs.Dataset(\n",
    "    {\n",
    "        \"GRU\": ([\"subbasin\", \"gru\"], lc_frac.values),\n",
    "        \"LandUse\": ([\"gru\"], lc_type),\n",
    "    },\n",
    "    coords={\n",
    "        \"lon\": ([\"subbasin\"], lon),\n",
    "        \"lat\": ([\"subbasin\"], lat),\n",
    "        \"time\": tt,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Meta data attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_ds.attrs['Conventions'] = 'CF-1.6'\n",
    "lc_ds.attrs['License']     = 'The data were written by Ala Bahrami'\n",
    "lc_ds.attrs['history']     = 'Created on April 23, 2021'\n",
    "lc_ds.attrs['featureType'] = 'point'          \n",
    "\n",
    "# editing lat attribute\n",
    "lc_ds['lat'].attrs['standard_name'] = 'latitude'\n",
    "lc_ds['lat'].attrs['units'] = 'degrees_north'\n",
    "lc_ds['lat'].attrs['axis'] = 'Y'\n",
    " \n",
    "# editing lon attribute\n",
    "lc_ds['lon'].attrs['standard_name'] = 'longitude'\n",
    "lc_ds['lon'].attrs['units'] = 'degrees_east'\n",
    "lc_ds['lon'].attrs['axis'] = 'X'\n",
    "\n",
    "# editing time attribute\n",
    "lc_ds['time'].attrs.update(standard_name = 'time', \n",
    "                                 units = ('days since %s 00:00:00' % date.today().strftime('%Y-%m-%d')), \n",
    "                                 axis = 'T')\n",
    "\n",
    "# coordinate system\n",
    "lc_ds['crs'] = drainage_db['crs'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Append land cover information to existing drainage database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drainage_db[\"GRU\"] = ([\"subbasin\", \"gru\"], lc_frac.values)\n",
    "drainage_db['GRU'].attrs['standard_name'] = 'GRU'\n",
    "drainage_db['GRU'].attrs['long_name'] = 'Group Response Unit'\n",
    "drainage_db['GRU'].attrs['units'] = '-'\n",
    "drainage_db['GRU'].attrs['_FillValue'] = -1\n",
    "\n",
    "drainage_db[\"LandUse\"] = ([\"gru\"], lc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set the 'coords' of the dataset to the new axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drainage_db = drainage_db.set_coords(['time', 'lon', 'lat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the drainage database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDDB = domain_name+'_MESH_drainage_database.nc'\n",
    "drainage_db.to_netcdf(outdir/outDDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (OPTIONAL) Save land cover fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFRAC = domain_name+'_MESH_LC_FRAC.nc'\n",
    "lc_ds.to_netcdf(outdir/outFRAC)\n",
    "print('--Completed in %s seconds--' %(time.time() - start_time))\n",
    "print('Drainage Database saved to {}'.format(outdir/outDDB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Provenance\n",
    "Generates a basic log file in the domain folder and copies the control file and itself there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log path and file name\n",
    "logPath = outdir\n",
    "log_suffix = '_create_MESH_drainage_database.txt'\n",
    " \n",
    "# Create a log folder\n",
    "logFolder = '_workflow_log'\n",
    "Path( logPath / logFolder ).mkdir(parents=True, exist_ok=True)\n",
    " \n",
    "# Copy this script\n",
    "thisFile = 'create_MESH_drainage_database.ipynb'\n",
    "copyfile(thisFile, logPath / logFolder / thisFile);\n",
    " \n",
    "# Get current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "# Create a log file\n",
    "logFile = now.strftime('%Y%m%d') + log_suffix\n",
    "with open( logPath / logFolder / logFile, 'w') as file:\n",
    "     \n",
    "    lines = ['Log generated by ' + thisFile + ' on ' + now.strftime('%Y/%m/%d %H:%M:%S') + '\\n',\n",
    "             'Generated drainage database .nc file.']\n",
    "    for txt in lines:\n",
    "        file.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MESH Workflow Kernel",
   "language": "python",
   "name": "mesh-workflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9040f0c7dac450993624bf71c867f85c56d04cf27b37d45a083321dcd6d7a2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
